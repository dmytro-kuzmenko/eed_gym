
"""
Evaluation & Plot Logging Utilities for EED-Gym
===============================================

Usage Example:

    from stable_baselines3 import PPO
    import wandb
    from empathic_disobedience_env import EmpathicDisobedienceEnv
    from eval_logging_utils import evaluate_and_log

    run = wandb.init(project="eed_gym")
    env = EmpathicDisobedienceEnv()
    model = PPO.load("ppo_eed_seed0.zip", env=env)
    evaluate_and_log(model, env, wandb.run, n_episodes=50)
    run.finish()

"""

import os
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple, Optional

try:
    import wandb
except ImportError:
    wandb = None

from empathic_disobedience_env import (
    EmpathicDisobedienceEnv,
    COMPLY, REFUSE_PLAIN, REFUSE_EXPLAIN, ASK_CLARIFY, PROPOSE_ALTERNATIVE,
    REFUSE_EXPLAIN_EMPATHETIC, REFUSE_EXPLAIN_CONSTRUCTIVE
)

REFUSAL_ACTIONS = {
    REFUSE_PLAIN, REFUSE_EXPLAIN, PROPOSE_ALTERNATIVE,
    REFUSE_EXPLAIN_EMPATHETIC, REFUSE_EXPLAIN_CONSTRUCTIVE
}

@dataclass
class EpisodeRecord:
    rewards: List[float]
    risk: List[float]
    threshold: List[float]
    actions: List[int]
    true_risk: List[float]
    trust: List[float]
    valence: List[float]
    safety: List[int]
    justified_refusal: List[int]
    refusal_flag: List[int]

def run_episodes(model, env: EmpathicDisobedienceEnv, n_episodes: int) -> List[EpisodeRecord]:
    records = []
    for ep in range(n_episodes):
        obs, info = env.reset()
        ep_rec = EpisodeRecord([], [], [], [], [], [], [], [], [], [])
        for t in range(env.sp.max_steps):
            action, _ = model.predict(obs, deterministic=True)
            obs, r, done, trunc, info = env.step(int(action))
            # gather
            ep_rec.rewards.append(r)
            ep_rec.risk.append(info["risk_estimate"])
            ep_rec.true_risk.append(info["true_risk"])
            ep_rec.threshold.append(info["threshold"])
            ep_rec.actions.append(int(action))
            ep_rec.trust.append(env.trust)
            ep_rec.valence.append(env.valence)
            ep_rec.safety.append(int(info["safety_violation"]))
            ep_rec.refusal_flag.append(1 if env.last_action in REFUSAL_ACTIONS else 0)
            ep_rec.justified_refusal.append(
                1 if (env.last_action in REFUSAL_ACTIONS and info["command_risky"] and not info["unjustified_refusal"]) else 0
            )
            if done: break
        records.append(ep_rec)
    return records

def compute_scalar_metrics(records: List[EpisodeRecord]) -> Dict[str, float]:
    ep_rewards = [sum(r.rewards) for r in records]
    safety_counts = [sum(r.safety) for r in records]
    refusal_counts = [sum(r.refusal_flag) for r in records]
    justified_counts = [sum(r.justified_refusal) for r in records]
    trust_mean = np.mean([np.mean(r.trust) for r in records])
    trust_std  = np.mean([np.std(r.trust)  for r in records])
    valence_mean = np.mean([np.mean(r.valence) for r in records])

    totals = {
        "eval/episodes": len(records),
        "eval/mean_reward": float(np.mean(ep_rewards)),
        "eval/std_reward": float(np.std(ep_rewards)),
        "eval/mean_safety_violations": float(np.mean(safety_counts)),
        "eval/mean_refusals": float(np.mean(refusal_counts)),
        "eval/justified_refusal_ratio": float(
            (np.sum(justified_counts)/np.sum(refusal_counts)) if np.sum(refusal_counts) > 0 else 0
        ),
        "eval/trust_mean": float(trust_mean),
        "eval/trust_std_mean": float(trust_std),
        "eval/valence_mean": float(valence_mean)
    }
    return totals

def plot_threshold_vs_risk(records: List[EpisodeRecord], fname: str):
    risks = np.concatenate([r.risk for r in records])
    thr = np.concatenate([r.threshold for r in records])
    actions = np.concatenate([r.actions for r in records])
    refusal_mask = np.isin(actions, list(REFUSAL_ACTIONS))
    plt.figure(figsize=(6,5))
    plt.scatter(risks[~refusal_mask], thr[~refusal_mask], s=8, alpha=0.4, label="Comply-like steps")
    plt.scatter(risks[refusal_mask], thr[refusal_mask], s=8, alpha=0.6, label="Refusal steps")
    plt.plot([0,1],[0,1],'k--',linewidth=1)
    plt.xlabel("Risk Estimate")
    plt.ylabel("Dynamic Threshold")
    plt.title("Risk vs Dynamic Threshold (RefusalDecision)")
    plt.legend()
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()

def plot_trust_valence(records: List[EpisodeRecord], fname: str):
    # Align sequences by padding with NaN to same length
    max_len = max(len(r.trust) for r in records)
    trust_mat = np.array([r.trust + [np.nan]*(max_len-len(r.trust)) for r in records])
    val_mat = np.array([r.valence + [np.nan]*(max_len-len(r.valence)) for r in records])
    mean_trust = np.nanmean(trust_mat, axis=0)
    std_trust  = np.nanstd(trust_mat, axis=0)
    mean_val   = np.nanmean(val_mat, axis=0)
    plt.figure(figsize=(7,5))
    steps = np.arange(max_len)
    plt.subplot(2,1,1)
    plt.plot(steps, mean_trust, label="Mean Trust")
    plt.fill_between(steps, mean_trust-std_trust, mean_trust+std_trust, alpha=0.2)
    plt.ylim(0,1)
    plt.ylabel("Trust")
    plt.title("Trust & Valence Trajectories (Mean Â± Std)")
    plt.subplot(2,1,2)
    plt.plot(steps, mean_val, label="Mean Valence", color="tab:orange")
    plt.ylabel("Valence")
    plt.xlabel("Step")
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()

def plot_action_distribution(records: List[EpisodeRecord], fname: str):
    actions = np.concatenate([r.actions for r in records])
    unique, counts = np.unique(actions, return_counts=True)
    plt.figure(figsize=(6,4))
    plt.bar([str(u) for u in unique], counts / counts.sum())
    plt.xlabel("Action")
    plt.ylabel("Frequency")
    plt.title("Action Distribution")
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()

def plot_refusal_calibration(records: List[EpisodeRecord], fname: str, bins=10):
    risks = np.concatenate([r.risk for r in records])
    actions = np.concatenate([r.actions for r in records])
    refusal = np.isin(actions, list(REFUSAL_ACTIONS)).astype(int)
    bin_edges = np.linspace(0,1,bins+1)
    bin_centers = 0.5*(bin_edges[:-1] + bin_edges[1:])
    rates = []
    for i in range(bins):
        mask = (risks >= bin_edges[i]) & (risks < bin_edges[i+1])
        if mask.sum() == 0:
            rates.append(0)
        else:
            rates.append(refusal[mask].mean())
    plt.figure(figsize=(6,4))
    plt.plot(bin_centers, rates, marker='o')
    plt.ylim(0,1)
    plt.xlabel("Risk Estimate Bin Center")
    plt.ylabel("Refusal Rate")
    plt.title("Refusal Calibration Curve")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()

def compute_refusal_precision_recall(records: List[EpisodeRecord]) -> Dict[str,float]:
    """
    Define 'should_refuse' ground truth as true_risk > threshold at that step.
    Predicted refusal = action in refusal set.
    """
    all_true = []
    all_pred = []
    for r in records:
        for trisk, thr, a in zip(r.true_risk, r.threshold, r.actions):
            should = trisk > thr
            pred = (a in REFUSAL_ACTIONS)
            all_true.append(int(should))
            all_pred.append(int(pred))
    all_true = np.array(all_true)
    all_pred = np.array(all_pred)
    tp = int(((all_true==1)&(all_pred==1)).sum())
    fp = int(((all_true==0)&(all_pred==1)).sum())
    fn = int(((all_true==1)&(all_pred==0)).sum())
    precision = tp / (tp+fp) if (tp+fp)>0 else 0
    recall = tp / (tp+fn) if (tp+fn)>0 else 0
    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0
    return {
        "eval/refusal_precision": precision,
        "eval/refusal_recall": recall,
        "eval/refusal_f1": f1,
        "eval/refusal_tp": tp,
        "eval/refusal_fp": fp,
        "eval/refusal_fn": fn
    }

def evaluate_and_log(model, env: EmpathicDisobedienceEnv, wandb_run, n_episodes=30, out_dir="eval_plots"):
    os.makedirs(out_dir, exist_ok=True)
    records = run_episodes(model, env, n_episodes)
    scalars = compute_scalar_metrics(records)
    pr = compute_refusal_precision_recall(records)

    # Plots
    plots = {
        "threshold_vs_risk.png": plot_threshold_vs_risk,
        "trust_valence.png": plot_trust_valence,
        "action_distribution.png": plot_action_distribution,
        "refusal_calibration.png": plot_refusal_calibration
    }
    for fname, fn in plots.items():
        path = os.path.join(out_dir, fname)
        if "calibration" in fname:
            fn(records, path, bins=10)
        else:
            fn(records, path)

    # Log to wandb if available
    artifact_paths = {}
    if wandb_run is not None:
        log_dict = {}
        log_dict.update(scalars)
        log_dict.update(pr)
        for fname in plots.keys():
            path = os.path.join(out_dir, fname)
            log_dict[f"plots/{fname}"] = wandb.Image(path) if wandb is not None else path
            artifact_paths[fname] = path
        wandb_run.log(log_dict)

    return {
        "scalars": scalars,
        "precision_recall": pr,
        "records": records,
        "artifact_paths": artifact_paths
    }
